{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('tree').getOrCreate()\n",
    "df = spark.read.csv('crop_recommendation.csv', inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "      <td>74</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>42</td>\n",
       "      <td>58</td>\n",
       "      <td>55</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>20.8797</td>\n",
       "      <td>21.7705</td>\n",
       "      <td>23.0045</td>\n",
       "      <td>26.4911</td>\n",
       "      <td>20.1302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>82.0027</td>\n",
       "      <td>80.3196</td>\n",
       "      <td>82.3208</td>\n",
       "      <td>80.1584</td>\n",
       "      <td>81.6049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ph</th>\n",
       "      <td>6.50299</td>\n",
       "      <td>7.0381</td>\n",
       "      <td>7.84021</td>\n",
       "      <td>6.9804</td>\n",
       "      <td>7.62847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainfall</th>\n",
       "      <td>202.936</td>\n",
       "      <td>226.656</td>\n",
       "      <td>263.964</td>\n",
       "      <td>242.864</td>\n",
       "      <td>262.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0        1        2        3        4\n",
       "N                 90       85       60       74       78\n",
       "P                 42       58       55       35       42\n",
       "K                 43       41       44       40       42\n",
       "temperature  20.8797  21.7705  23.0045  26.4911  20.1302\n",
       "humidity     82.0027  80.3196  82.3208  80.1584  81.6049\n",
       "ph           6.50299   7.0381  7.84021   6.9804  7.62847\n",
       "rainfall     202.936  226.656  263.964  242.864  262.717\n",
       "label           rice     rice     rice     rice     rice"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(df.take(5), columns = df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall', 'label']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------------+------------------+------------------+-----+--------------------+-----------+\n",
      "|  N|  P|  K|temperature|         humidity|                ph|          rainfall|label|            features|genderIndex|\n",
      "+---+---+---+-----------+-----------------+------------------+------------------+-----+--------------------+-----------+\n",
      "| 90| 42| 43|20.87974371|      82.00274423| 6.502985292000001|       202.9355362| rice|[90.0,42.0,43.0,2...|       20.0|\n",
      "| 85| 58| 41|21.77046169|      80.31964408|       7.038096361|       226.6555374| rice|[85.0,58.0,41.0,2...|       20.0|\n",
      "| 60| 55| 44|23.00445915|       82.3207629|       7.840207144|       263.9642476| rice|[60.0,55.0,44.0,2...|       20.0|\n",
      "| 74| 35| 40|26.49109635|      80.15836264|       6.980400905|       242.8640342| rice|[74.0,35.0,40.0,2...|       20.0|\n",
      "| 78| 42| 42|20.13017482|      81.60487287|       7.628472891|       262.7173405| rice|[78.0,42.0,42.0,2...|       20.0|\n",
      "| 69| 37| 42|23.05804872|      83.37011772|       7.073453503|       251.0549998| rice|[69.0,37.0,42.0,2...|       20.0|\n",
      "| 69| 55| 38|22.70883798|      82.63941394|        5.70080568|       271.3248604| rice|[69.0,55.0,38.0,2...|       20.0|\n",
      "| 94| 53| 40|20.27774362|      82.89408619| 5.718627177999999|       241.9741949| rice|[94.0,53.0,40.0,2...|       20.0|\n",
      "| 89| 54| 38|24.51588066|83.53521629999999|       6.685346424|       230.4462359| rice|[89.0,54.0,38.0,2...|       20.0|\n",
      "| 68| 58| 38|23.22397386|      83.03322691|       6.336253525|       221.2091958| rice|[68.0,58.0,38.0,2...|       20.0|\n",
      "| 91| 53| 40|26.52723513|      81.41753846|       5.386167788|       264.6148697| rice|[91.0,53.0,40.0,2...|       20.0|\n",
      "| 90| 46| 42|23.97898217|      81.45061596|        7.50283396|       250.0832336| rice|[90.0,46.0,42.0,2...|       20.0|\n",
      "| 78| 58| 44|26.80079604|      80.88684822|       5.108681786|       284.4364567| rice|[78.0,58.0,44.0,2...|       20.0|\n",
      "| 93| 56| 36|24.01497622|      82.05687182|        6.98435366|       185.2773389| rice|[93.0,56.0,36.0,2...|       20.0|\n",
      "| 94| 50| 37|25.66585205|      80.66385045|        6.94801983|       209.5869708| rice|[94.0,50.0,37.0,2...|       20.0|\n",
      "| 60| 48| 39|24.28209415|      80.30025587|7.0422990689999985|       231.0863347| rice|[60.0,48.0,39.0,2...|       20.0|\n",
      "| 85| 38| 41|21.58711777|       82.7883708|6.2490506560000005|276.65524589999995| rice|[85.0,38.0,41.0,2...|       20.0|\n",
      "| 91| 35| 39|23.79391957|      80.41817957|       6.970859754|       206.2611855| rice|[91.0,35.0,39.0,2...|       20.0|\n",
      "| 77| 38| 36| 21.8652524|       80.1923008|       5.953933276|224.55501690000003| rice|[77.0,38.0,36.0,2...|       20.0|\n",
      "| 88| 35| 40|23.57943626|      83.58760316|        5.85393208| 291.2986618000001| rice|[88.0,35.0,40.0,2...|       20.0|\n",
      "+---+---+---+-----------+-----------------+------------------+------------------+-----+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "gender_indexer = StringIndexer(inputCol=\"label\", outputCol=\"genderIndex\")\n",
    "#Fits a model to the input dataset with optional parameters.\n",
    "df = gender_indexer.fit(output).transform(output)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|            features|genderIndex|\n",
      "+--------------------+-----------+\n",
      "|[90.0,42.0,43.0,2...|       20.0|\n",
      "|[85.0,58.0,41.0,2...|       20.0|\n",
      "|[60.0,55.0,44.0,2...|       20.0|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = df.select('features', 'genderIndex')\n",
    "final_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import (DecisionTreeClassifier, RandomForestClassifier)\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol = 'genderIndex', featuresCol = 'features')\n",
    "rf = RandomForestClassifier(labelCol = 'genderIndex', featuresCol = 'features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(train)\n",
    "rf_model = rf.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predictions = dt_model.transform(test)\n",
    "rf_predictions = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: 0.35711777358196445\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "binary_evaluator = MulticlassClassificationEvaluator(labelCol = 'genderIndex')\n",
    "\n",
    "print('Decision Tree:', binary_evaluator.evaluate(dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accu: 0.4119318181818182\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'genderIndex', metricName = 'accuracy')\n",
    "print('Decision Tree Accu:', multi_evaluator.evaluate(dt_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: 0.9787207528591864\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest:' , binary_evaluator.evaluate(rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accu: 0.9786931818181818\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Accu:', multi_evaluator.evaluate(rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.read.csv('crop_recommendation.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|            features|species|\n",
      "+--------------------+-------+\n",
      "|[90.0,42.0,43.0,2...|   rice|\n",
      "|[85.0,58.0,41.0,2...|   rice|\n",
      "|[60.0,55.0,44.0,2...|   rice|\n",
      "|[74.0,35.0,40.0,2...|   rice|\n",
      "|[78.0,42.0,42.0,2...|   rice|\n",
      "+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris2 = iris.rdd.map(lambda x: Row(features=Vectors.dense(x[:-1]), species=x[-1])).toDF()\n",
    "iris2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringindexer = StringIndexer(inputCol='species', outputCol='label')\n",
    "stages = [stringindexer]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+-----+\n",
      "|            features|species|label|\n",
      "+--------------------+-------+-----+\n",
      "|[90.0,42.0,43.0,2...|   rice| 20.0|\n",
      "|[85.0,58.0,41.0,2...|   rice| 20.0|\n",
      "|[60.0,55.0,44.0,2...|   rice| 20.0|\n",
      "|[74.0,35.0,40.0,2...|   rice| 20.0|\n",
      "|[78.0,42.0,42.0,2...|   rice| 20.0|\n",
      "+--------------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df = pipeline.fit(iris2).transform(iris2)\n",
    "iris_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+\n",
      "|summary|   species|            label|\n",
      "+-------+----------+-----------------+\n",
      "|  count|      2200|             2200|\n",
      "|   mean|      null|             10.5|\n",
      "| stddev|      null|6.345731145773738|\n",
      "|    min|     apple|              0.0|\n",
      "|    max|watermelon|             21.0|\n",
      "+-------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_df.describe().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('features', 'vector'), ('species', 'string'), ('label', 'double')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = iris_df.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "naivebayes = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(naivebayes.smoothing, [0, 1, 2, 4, 8]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "crossvalidator = CrossValidator(estimator=naivebayes, estimatorParamMaps=param_grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalidation_mode = crossvalidator.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "|            features|    species|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,5.0,36.0,24....|pomegranate| 19.0|[-499.38742474583...|[6.61408756932272...|      19.0|\n",
      "|[0.0,17.0,30.0,35...|      mango| 12.0|[-455.27347844735...|[5.14165896370608...|      12.0|\n",
      "|[0.0,17.0,42.0,23...|pomegranate| 19.0|[-529.65008083143...|[1.45592672238025...|      19.0|\n",
      "|[0.0,19.0,31.0,25...|    coconut|  4.0|[-646.35885666477...|[1.01337421998693...|       4.0|\n",
      "|[0.0,19.0,33.0,27...|    coconut|  4.0|[-698.05975276499...|[3.09992240756038...|       4.0|\n",
      "+--------------------+-----------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_train = crossvalidation_mode.transform(train)\n",
    "pred_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "|            features|   species|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,12.0,7.0,20....|    orange| 16.0|[-486.81372144990...|[2.75828665326150...|      16.0|\n",
      "|[0.0,18.0,14.0,29...|    orange| 16.0|[-534.16811816168...|[5.32634657366113...|      16.0|\n",
      "|[0.0,70.0,21.0,36...|pigeonpeas| 18.0|[-540.87173626541...|[8.00169243937490...|      18.0|\n",
      "|[0.0,133.0,200.0,...|     apple|  0.0|[-859.64981906109...|[0.99998891838687...|       0.0|\n",
      "|[0.0,137.0,195.0,...|    grapes|  7.0|[-775.85990522712...|[0.01694944491566...|       7.0|\n",
      "+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_test = crossvalidation_mode.transform(test)\n",
    "pred_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameter smoothing has best value: 8.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The parameter smoothing has best value:\",\n",
    "      crossvalidation_mode.bestModel._java_obj.getSmoothing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data (f1): 0.8883428368172839 \n",
      " training data (weightedPrecision):  0.8916673502536663 \n",
      " training data (weightedRecall):  0.8905191873589166 \n",
      " training data (accuracy):  0.8905191873589164\n"
     ]
    }
   ],
   "source": [
    "print('training data (f1):', evaluator.setMetricName('f1').evaluate(pred_train), \"\\n\",\n",
    "     'training data (weightedPrecision): ', evaluator.setMetricName('weightedPrecision').evaluate(pred_train),\"\\n\",\n",
    "     'training data (weightedRecall): ', evaluator.setMetricName('weightedRecall').evaluate(pred_train),\"\\n\",\n",
    "     'training data (accuracy): ', evaluator.setMetricName('accuracy').evaluate(pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data (f1): 0.8803898971666413 \n",
      " test data (weightedPrecision):  0.8859065599572711 \n",
      " test data (weightedRecall):  0.8808411214953269 \n",
      " test data (accuracy):  0.8808411214953271\n"
     ]
    }
   ],
   "source": [
    "print('test data (f1):', evaluator.setMetricName('f1').evaluate(pred_test), \"\\n\",\n",
    "     'test data (weightedPrecision): ', evaluator.setMetricName('weightedPrecision').evaluate(pred_test),\"\\n\",\n",
    "     'test data (weightedRecall): ', evaluator.setMetricName('weightedRecall').evaluate(pred_test),\"\\n\",\n",
    "     'test data (accuracy): ', evaluator.setMetricName('accuracy').evaluate(pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import * \n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, asc,desc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler,StandardScaler #onehotencoder\n",
    "from pyspark.ml import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "spark=SparkSession.builder \\\n",
    ".master (\"local[*]\")\\\n",
    ".appName(\"part3\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\BigDataLocalSetup\\Spark\\python\\pyspark\\sql\\context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2200 rows 8 columns in the data.\n"
     ]
    }
   ],
   "source": [
    "df=spark.read \\\n",
    " .option(\"header\",\"True\")\\\n",
    " .option(\"inferSchema\",\"True\")\\\n",
    " .option(\"sep\",\",\")\\\n",
    " .csv(\"crop_recommendation.csv\")\n",
    "print(\"There are\",df.count(),\"rows\",len(df.columns),\n",
    "      \"columns\" ,\"in the data.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "|  N|  P|  K|temperature|   humidity|               ph|   rainfall|label|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "| 90| 42| 43|20.87974371|82.00274423|6.502985292000001|202.9355362| rice|\n",
      "| 85| 58| 41|21.77046169|80.31964408|      7.038096361|226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915| 82.3207629|      7.840207144|263.9642476| rice|\n",
      "| 74| 35| 40|26.49109635|80.15836264|      6.980400905|242.8640342| rice|\n",
      "+---+---+---+-----------+-----------+-----------------+-----------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- N: integer (nullable = true)\n",
      " |-- P: integer (nullable = true)\n",
      " |-- K: integer (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- ph: double (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>2200</td>\n",
       "      <td>50.551818181818184</td>\n",
       "      <td>36.917333833756594</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>2200</td>\n",
       "      <td>53.36272727272727</td>\n",
       "      <td>32.98588273858713</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>2200</td>\n",
       "      <td>48.14909090909091</td>\n",
       "      <td>50.647930546660135</td>\n",
       "      <td>5</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0                   1                   2    3    4\n",
       "summary  count                mean              stddev  min  max\n",
       "N         2200  50.551818181818184  36.917333833756594    0  140\n",
       "P         2200   53.36272727272727   32.98588273858713    5  145\n",
       "K         2200   48.14909090909091  50.647930546660135    5  205"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']\n",
    "df.select(numeric_features).describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>20.879744</td>\n",
       "      <td>82.002744</td>\n",
       "      <td>6.502985</td>\n",
       "      <td>202.935536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>21.770462</td>\n",
       "      <td>80.319644</td>\n",
       "      <td>7.038096</td>\n",
       "      <td>226.655537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>23.004459</td>\n",
       "      <td>82.320763</td>\n",
       "      <td>7.840207</td>\n",
       "      <td>263.964248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>26.491096</td>\n",
       "      <td>80.158363</td>\n",
       "      <td>6.980401</td>\n",
       "      <td>242.864034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>20.130175</td>\n",
       "      <td>81.604873</td>\n",
       "      <td>7.628473</td>\n",
       "      <td>262.717340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    N   P   K  temperature   humidity        ph    rainfall\n",
       "0  90  42  43    20.879744  82.002744  6.502985  202.935536\n",
       "1  85  58  41    21.770462  80.319644  7.038096  226.655537\n",
       "2  60  55  44    23.004459  82.320763  7.840207  263.964248\n",
       "3  74  35  40    26.491096  80.158363  6.980401  242.864034\n",
       "4  78  42  42    20.130175  81.604873  7.628473  262.717340"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features = [t[0] for t in df.dtypes if t[1] != 'string']\n",
    "numeric_features_df=df.select(numeric_features)\n",
    "numeric_features_df.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>K</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>ph</th>\n",
       "      <th>rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.231460</td>\n",
       "      <td>-0.140512</td>\n",
       "      <td>0.026504</td>\n",
       "      <td>0.190688</td>\n",
       "      <td>0.096683</td>\n",
       "      <td>0.059020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>-0.231460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.736232</td>\n",
       "      <td>-0.127541</td>\n",
       "      <td>-0.118734</td>\n",
       "      <td>-0.138019</td>\n",
       "      <td>-0.063839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K</th>\n",
       "      <td>-0.140512</td>\n",
       "      <td>0.736232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.160387</td>\n",
       "      <td>0.190859</td>\n",
       "      <td>-0.169503</td>\n",
       "      <td>-0.053461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>0.026504</td>\n",
       "      <td>-0.127541</td>\n",
       "      <td>-0.160387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.205320</td>\n",
       "      <td>-0.017795</td>\n",
       "      <td>-0.030084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humidity</th>\n",
       "      <td>0.190688</td>\n",
       "      <td>-0.118734</td>\n",
       "      <td>0.190859</td>\n",
       "      <td>0.205320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>0.094423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ph</th>\n",
       "      <td>0.096683</td>\n",
       "      <td>-0.138019</td>\n",
       "      <td>-0.169503</td>\n",
       "      <td>-0.017795</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.109069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rainfall</th>\n",
       "      <td>0.059020</td>\n",
       "      <td>-0.063839</td>\n",
       "      <td>-0.053461</td>\n",
       "      <td>-0.030084</td>\n",
       "      <td>0.094423</td>\n",
       "      <td>-0.109069</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    N         P         K  temperature  humidity        ph  \\\n",
       "N            1.000000 -0.231460 -0.140512     0.026504  0.190688  0.096683   \n",
       "P           -0.231460  1.000000  0.736232    -0.127541 -0.118734 -0.138019   \n",
       "K           -0.140512  0.736232  1.000000    -0.160387  0.190859 -0.169503   \n",
       "temperature  0.026504 -0.127541 -0.160387     1.000000  0.205320 -0.017795   \n",
       "humidity     0.190688 -0.118734  0.190859     0.205320  1.000000 -0.008483   \n",
       "ph           0.096683 -0.138019 -0.169503    -0.017795 -0.008483  1.000000   \n",
       "rainfall     0.059020 -0.063839 -0.053461    -0.030084  0.094423 -0.109069   \n",
       "\n",
       "             rainfall  \n",
       "N            0.059020  \n",
       "P           -0.063839  \n",
       "K           -0.053461  \n",
       "temperature -0.030084  \n",
       "humidity     0.094423  \n",
       "ph          -0.109069  \n",
       "rainfall     1.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names =numeric_features_df.columns\n",
    "features = numeric_features_df.rdd.map(lambda row: row[0:])\n",
    "corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_mat)\n",
    "corr_df.index, corr_df.columns = col_names, col_names\n",
    "\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "|            features|   species|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,12.0,7.0,20....|    orange| 16.0|[-1.7137232221721...|[3.04189469945704...|      16.0|\n",
      "|[0.0,18.0,14.0,29...|    orange| 16.0|[-1.6160972464000...|[1.53232136807711...|      16.0|\n",
      "|[0.0,70.0,21.0,36...|pigeonpeas| 18.0|[-0.6452509925280...|[6.21267646738206...|      18.0|\n",
      "|[0.0,133.0,200.0,...|     apple|  0.0|[9.09563817393450...|[0.44443467772825...|       7.0|\n",
      "|[0.0,137.0,195.0,...|    grapes|  7.0|[7.65098269234096...|[0.08498656021339...|       7.0|\n",
      "|[0.0,145.0,205.0,...|     apple|  0.0|[9.95792200232084...|[0.54013068617806...|       0.0|\n",
      "|[1.0,6.0,35.0,27....|   coconut|  4.0|[-0.0788228004175...|[3.04407513179140...|       4.0|\n",
      "|[1.0,35.0,34.0,30...|     mango| 12.0|[-2.6372118600303...|[7.11453448613823...|      12.0|\n",
      "|[1.0,132.0,200.0,...|    grapes|  7.0|[8.29464683002434...|[0.15332513786507...|       7.0|\n",
      "|[2.0,21.0,35.0,25...|   coconut|  4.0|[0.75287236585726...|[9.05382267020572...|       4.0|\n",
      "|[2.0,36.0,31.0,30...|     mango| 12.0|[-2.3049175748087...|[8.391443012854E-...|      12.0|\n",
      "|[2.0,40.0,27.0,29...|     mango| 12.0|[-2.5277604292943...|[9.55986152880043...|      12.0|\n",
      "|[2.0,47.0,15.0,29...|  mungbean| 14.0|[-1.4657737681323...|[2.97336941940209...|      14.0|\n",
      "|[2.0,51.0,17.0,25...| mothbeans| 13.0|[-3.2485668545346...|[2.55666356648567...|      13.0|\n",
      "|[2.0,75.0,22.0,23...|    lentil| 10.0|[-1.5766845958814...|[2.12493466957813...|      10.0|\n",
      "|[2.0,120.0,203.0,...|     apple|  0.0|[8.94542972159804...|[0.53652660503771...|       0.0|\n",
      "|[2.0,140.0,197.0,...|     apple|  0.0|[9.44521579526675...|[0.49857235991870...|       0.0|\n",
      "|[2.0,143.0,196.0,...|     apple|  0.0|[9.40647612625860...|[0.49660007214726...|       0.0|\n",
      "|[3.0,23.0,30.0,29...|   coconut|  4.0|[1.39899069984654...|[2.85434989493890...|       4.0|\n",
      "|[3.0,63.0,16.0,24...|    lentil| 10.0|[-2.3272337466360...|[1.14327230156731...|      10.0|\n",
      "+--------------------+----------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=5)\n",
    "lrModel = lr.fit(train)\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.show()\n",
    "#predictions_train = lrModel.transform(train)\n",
    "# predictions.select('label', 'features',  'rawPrediction', 'prediction', 'probability').toPandas().head(5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8341121495327103\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "print(\"Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-----------+-----------------+------------------+------------------+-----+\n",
      "|_c0|_c1|_c2|        _c3|              _c4|               _c5|               _c6|  _c7|\n",
      "+---+---+---+-----------+-----------------+------------------+------------------+-----+\n",
      "|  N|  P|  K|temperature|         humidity|                ph|          rainfall|label|\n",
      "| 90| 42| 43|20.87974371|      82.00274423| 6.502985292000001|       202.9355362| rice|\n",
      "| 85| 58| 41|21.77046169|      80.31964408|       7.038096361|       226.6555374| rice|\n",
      "| 60| 55| 44|23.00445915|       82.3207629|       7.840207144|       263.9642476| rice|\n",
      "| 74| 35| 40|26.49109635|      80.15836264|       6.980400905|       242.8640342| rice|\n",
      "| 78| 42| 42|20.13017482|      81.60487287|       7.628472891|       262.7173405| rice|\n",
      "| 69| 37| 42|23.05804872|      83.37011772|       7.073453503|       251.0549998| rice|\n",
      "| 69| 55| 38|22.70883798|      82.63941394|        5.70080568|       271.3248604| rice|\n",
      "| 94| 53| 40|20.27774362|      82.89408619| 5.718627177999999|       241.9741949| rice|\n",
      "| 89| 54| 38|24.51588066|83.53521629999999|       6.685346424|       230.4462359| rice|\n",
      "| 68| 58| 38|23.22397386|      83.03322691|       6.336253525|       221.2091958| rice|\n",
      "| 91| 53| 40|26.52723513|      81.41753846|       5.386167788|       264.6148697| rice|\n",
      "| 90| 46| 42|23.97898217|      81.45061596|        7.50283396|       250.0832336| rice|\n",
      "| 78| 58| 44|26.80079604|      80.88684822|       5.108681786|       284.4364567| rice|\n",
      "| 93| 56| 36|24.01497622|      82.05687182|        6.98435366|       185.2773389| rice|\n",
      "| 94| 50| 37|25.66585205|      80.66385045|        6.94801983|       209.5869708| rice|\n",
      "| 60| 48| 39|24.28209415|      80.30025587|7.0422990689999985|       231.0863347| rice|\n",
      "| 85| 38| 41|21.58711777|       82.7883708|6.2490506560000005|276.65524589999995| rice|\n",
      "| 91| 35| 39|23.79391957|      80.41817957|       6.970859754|       206.2611855| rice|\n",
      "| 77| 38| 36| 21.8652524|       80.1923008|       5.953933276|224.55501690000003| rice|\n",
      "+---+---+---+-----------+-----------------+------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fileName = \"crop_recommendation.csv\"\n",
    "dataDF = spark.read.format(\"csv\").option(\"numFeatures\", \"784\").load(fileName)\n",
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: _c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-df6447d59681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Fit the StandardScaler: learn the statistics of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mscalerModel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Normalize the data: apply the normalization transformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigDataLocalSetup\\Spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\BigDataLocalSetup\\Spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigDataLocalSetup\\Spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigDataLocalSetup\\Spark\\python\\lib\\py4j-0.10.9.2-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigDataLocalSetup\\Spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: _c0, _c1, _c2, _c3, _c4, _c5, _c6, _c7"
     ]
    }
   ],
   "source": [
    "#Complete the #FILL IN# gaps\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Define the normalizer object: indicate that you only want each feature to have unit standard deviation. Use the nomenclature \"normFeatures\" for column with the output normalized features.\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"normFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "# Fit the StandardScaler: learn the statistics of the data\n",
    "scalerModel = scaler.fit(dataDF)\n",
    "\n",
    "# Normalize the data: apply the normalization transformation\n",
    "scaledData = scalerModel.transform(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
